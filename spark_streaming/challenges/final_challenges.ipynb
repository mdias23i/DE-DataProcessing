{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdias23i/DE-DataProcessing/blob/main/spark_streaming/challenges/final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "4488cdba-1814-466a-873d-5639f2ecbf49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udk3tohSaXOH",
        "outputId": "2711b63d-8089-4433-9390-0731c3cfb4f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n",
            "Downloading Faker-33.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree(\"content/lake/bronze/messages\", ignore_errors=True)\n",
        "shutil.rmtree(\"content/lake/silver/messages\", ignore_errors=True)\n",
        "shutil.rmtree(\"content/lake/silver/messages_corrupted\", ignore_errors=True)\n",
        "shutil.rmtree(\"content/lake/silver/checkpoint\", ignore_errors=True)"
      ],
      "metadata": {
        "id": "8TWVv8yWA0GC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tPCOdivrfhYh"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "import shutil\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "shutil.rmtree(\"content/lake/bronze/messages\", ignore_errors=True)\n",
        "shutil.rmtree(\"content/lake/silver/messages\", ignore_errors=True)\n",
        "shutil.rmtree(\"content/lake/silver/messages_corrupted\", ignore_errors=True)\n",
        "\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "      'event_status': F.lit(fake.random_element(elements=('SUCCESS', 'FAILED', 'PENDING', 'IN_PROGRESS', 'CANCELLED', \"NONE\", \"\"))),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".option('checkpointLocation', 'content/lake/bronze/messages/checkpoint')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "# Run for at least 1 minute\n",
        "query.awaitTermination(60)\n",
        "\n",
        "# Stop the query\n",
        "query.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for stream in spark.streams.active:\n",
        "    stream.stop()"
      ],
      "metadata": {
        "id": "P9dcXPmhCFY0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZWQExsnzlMFe",
        "outputId": "9e0d0dfc-7b4f-40ee-a75d-3c501e9909b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/content/lake/silver/messages/data/*.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-71da7483303c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content/lake/silver/messages/data/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/content/lake/silver/messages/data/*."
          ]
        }
      ],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data/*\")\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "countries.show()"
      ],
      "metadata": {
        "id": "HiECJL3L2OhL",
        "outputId": "da4e3a44-6362-44db-c47c-55a87a5f1bd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----------+\n",
            "|       country|country_id|\n",
            "+--------------+----------+\n",
            "|        Brazil|      2000|\n",
            "|      Portugal|      2001|\n",
            "|         Spain|      2002|\n",
            "|       Germany|      2003|\n",
            "|        France|      2004|\n",
            "|         Italy|      2005|\n",
            "|United Kingdom|      2006|\n",
            "| United States|      2007|\n",
            "|        Canada|      2008|\n",
            "|     Australia|      2009|\n",
            "|         Japan|      2010|\n",
            "|         China|      2011|\n",
            "|         India|      2012|\n",
            "|   South Korea|      2013|\n",
            "|        Russia|      2014|\n",
            "|     Argentina|      2015|\n",
            "+--------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg2nx03_Sn62"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZAHIZeZMlpoH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "#Setting schemas\n",
        "message_schema = StructType([\n",
        "    StructField(\"event_status\", StringType(), True),\n",
        "    StructField(\"event_type\", StringType(), True),\n",
        "    StructField(\"message_id\", StringType(), True),\n",
        "    StructField(\"channel\", StringType(), True),\n",
        "    StructField(\"country_id\", IntegerType(), True),\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "countries_schema = StructType([\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"country_id\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "# reading from Bronze layer\n",
        "bronze_stream = (spark.readStream\n",
        "                 .schema(message_schema)\n",
        "                 .format(\"parquet\")\n",
        "                 .load(\"content/lake/bronze/messages/data\"))\n",
        "\n",
        "\n",
        "#Function to process each batch\n",
        "def process_batch(df, batch_id):\n",
        "\n",
        "  df_with_country = df.join(countries, on=\"country_id\")\n",
        "\n",
        "  df_with_country = df_with_country.withColumn(\"date\", F.to_date(\"timestamp\"))\n",
        "\n",
        "  df_country_corrupted = df_with_country.filter(\n",
        "        (F.col(\"event_status\").isNull()) |\n",
        "        (F.col(\"event_status\") == \"\") |\n",
        "        (F.col(\"event_status\") == \"NONE\")\n",
        "    )\n",
        "  df_country_non_corrupted = df_with_country.filter(\n",
        "        ~((F.col(\"event_status\").isNull()) |\n",
        "          (F.col(\"event_status\") == \"\") |\n",
        "          (F.col(\"event_status\") == \"NONE\"))\n",
        "    )\n",
        "\n",
        "    # Write corrupted data\n",
        "  (df_country_corrupted.write\n",
        "     .mode(\"append\")\n",
        "     .partitionBy(\"date\")\n",
        "     .format(\"parquet\")\n",
        "     .save(\"content/lake/silver/messages_corrupted/data\"))\n",
        "\n",
        "    # Write not corrupted data\n",
        "  (df_country_non_corrupted.write\n",
        "     .mode(\"append\")\n",
        "     .partitionBy(\"date\")\n",
        "     .format(\"parquet\")\n",
        "     .save(\"content/lake/silver/messages/data\"))\n",
        "\n",
        "\n",
        "\n",
        "query2 = (bronze_stream.writeStream\n",
        "         .outputMode(\"append\")\n",
        "         .foreachBatch(process_batch)\n",
        "         .option(\"checkpointLocation\", \"content/lake/silver/checkpoint\")\n",
        "         .trigger(processingTime=\"5 seconds\")\n",
        "         .start())\n",
        "\n",
        "\n",
        "query2.awaitTermination(20)\n",
        "\n",
        "query2.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nk8seEvbmvcU",
        "outputId": "5337ad0a-f7ea-43db-9e3a-3485f0631713",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match\n"
          ]
        }
      ],
      "source": [
        "bronze_df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data\")\n",
        "bronze_count = bronze_df.count()\n",
        "\n",
        "messages_df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data\")\n",
        "silver_messages_count = messages_df.count()\n",
        "\n",
        "corrupted_df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages_corrupted/data\")\n",
        "silver_corrupted_count = corrupted_df.count()\n",
        "\n",
        "total_silver_count = silver_messages_count + silver_corrupted_count\n",
        "\n",
        "if bronze_count == total_silver_count:\n",
        "    print(\"Results match\")\n",
        "else:\n",
        "    print(\"Error, results do not match\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "R3J9XyOHhqvU"
      },
      "outputs": [],
      "source": [
        "# dedup data\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data\")\n",
        "dedup = df.withColumn(\"row_number\", F.row_number().over(Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\"))).filter(\"row_number = 1\").drop(\"row_number\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UPHSMSXnTKgu",
        "outputId": "d7415441-f15a-4de2-8c0a-b2b70678311a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|2024-12-13|   PUSH|      0|      1|   2|       0|   2|\n",
            "|2024-12-13|    SMS|      2|      1|   1|       0|   1|\n",
            "|2024-12-13|   CHAT|      4|      1|   0|       0|   2|\n",
            "|2024-12-13|  EMAIL|      1|      0|   0|       2|   0|\n",
            "|2024-12-13|  OTHER|      2|      0|   0|       3|   4|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# report 1\n",
        "report1 = (\n",
        "    dedup.groupBy(F.col(\"date\"), F.col(\"channel\"))\n",
        "    .pivot(\"event_type\", [\"CLICKED\", \"CREATED\", \"OPEN\", \"RECEIVED\", \"SENT\"])\n",
        "    .count()\n",
        "    .fillna(0)\n",
        ")\n",
        "\n",
        "# Mostrar o relatório 1\n",
        "report1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rsS7bkAJmWsW",
        "outputId": "1aaa6e55-ed06-4920-8883-dcb4dca05df5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+-----+----+---+----------+\n",
            "|user_id|CHAT|EMAIL|OTHER|PUSH|SMS|iterations|\n",
            "+-------+----+-----+-----+----+---+----------+\n",
            "|   1010|   0|    0|    1|   1|  1|         3|\n",
            "|   1049|   1|    0|    2|   0|  0|         3|\n",
            "|   1014|   0|    0|    1|   1|  1|         3|\n",
            "|   1025|   0|    0|    1|   0|  1|         2|\n",
            "|   1021|   1|    0|    1|   0|  0|         2|\n",
            "|   1026|   1|    0|    0|   1|  0|         2|\n",
            "|   1032|   1|    0|    0|   1|  0|         2|\n",
            "|   1020|   1|    0|    0|   0|  1|         2|\n",
            "|   1003|   2|    0|    0|   0|  0|         2|\n",
            "|   1040|   0|    1|    0|   0|  1|         2|\n",
            "|   1005|   0|    0|    1|   0|  0|         1|\n",
            "|   1047|   0|    0|    0|   1|  0|         1|\n",
            "|   1028|   0|    1|    0|   0|  0|         1|\n",
            "|   1050|   1|    0|    0|   0|  0|         1|\n",
            "|   1035|   0|    0|    0|   0|  1|         1|\n",
            "|   1045|   0|    0|    0|   1|  0|         1|\n",
            "|   1017|   1|    0|    0|   0|  0|         1|\n",
            "|   1036|   0|    1|    0|   0|  0|         1|\n",
            "|   1015|   0|    0|    0|   0|  1|         1|\n",
            "|   1022|   0|    0|    1|   0|  0|         1|\n",
            "+-------+----+-----+-----+----+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# report 2\n",
        "report2 = (\n",
        "    dedup.groupBy(F.col(\"user_id\"), F.col(\"channel\"))\n",
        "    .count()\n",
        "    .groupBy(\"user_id\")\n",
        "    .pivot(\"channel\", [\"CHAT\", \"EMAIL\", \"OTHER\", \"PUSH\", \"SMS\"])\n",
        "    .sum(\"count\")\n",
        "    .fillna(0)\n",
        ")\n",
        "\n",
        "# Adding iterations column\n",
        "report2 = report2.withColumn(\"iterations\", sum(report2[col] for col in [\"CHAT\", \"EMAIL\", \"OTHER\", \"PUSH\", \"SMS\"]))\n",
        "\n",
        "# Ordering by iteration\n",
        "report2 = report2.orderBy(F.desc(\"iterations\"))\n",
        "\n",
        "report2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef0RjFTxwE5y"
      },
      "outputs": [],
      "source": [
        "# Theoretical question:\n",
        "\n",
        "# A new usecase requires the message data to be aggregate in near real time\n",
        "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
        "# This application will access directly the data aggregated by streaming process\n",
        "\n",
        "# Q1:\n",
        "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
        "Or would you choose a different data processing tool?\n",
        "\n",
        "- Which storage would you use and why? (database?, data lake?, kafka?)\n",
        "\n",
        "\n",
        "I would use Spark Structured Streaming to process data in time windows, for example, every minute.\n",
        "Kafka would be used to ingest the messages, and Spark would aggregate the data in real time using groupBy and window functions.\n",
        "The processed data would then be written to a database for quick access and real-time analytics, while the raw data would be stored in a data lake for potential future use.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree(\"content/lake/bronze/messages\", ignore_errors=True)\n",
        "shutil.rmtree(\"content/lake/silver/messages\", ignore_errors=True)\n",
        "shutil.rmtree(\"content/lake/silver/messages_corrupted\", ignore_errors=True)\n",
        "shutil.rmtree(\"content/lake/silver/checkpoint\", ignore_errors=True)\n",
        "\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "shutil.rmtree(\"content/lake/bronze/messages\", ignore_errors=True)\n",
        "shutil.rmtree(\"content/lake/silver/messages\", ignore_errors=True)\n",
        "shutil.rmtree(\"content/lake/silver/messages_corrupted\", ignore_errors=True)\n",
        "\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "      'event_status': F.lit(fake.random_element(elements=('SUCCESS', 'FAILED', 'PENDING', 'IN_PROGRESS', 'CANCELLED', \"NONE\", \"\"))),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".option('checkpointLocation', 'content/lake/bronze/messages/checkpoint')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "# Run for at least 1 minute\n",
        "query.awaitTermination(60)\n",
        "\n",
        "# Stop the query\n",
        "query.stop()\n",
        "\n",
        "\n",
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)\n",
        "\n",
        "\n",
        "#Setting schemas\n",
        "message_schema = StructType([\n",
        "    StructField(\"event_status\", StringType(), True),\n",
        "    StructField(\"event_type\", StringType(), True),\n",
        "    StructField(\"message_id\", StringType(), True),\n",
        "    StructField(\"channel\", StringType(), True),\n",
        "    StructField(\"country_id\", IntegerType(), True),\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "countries_schema = StructType([\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"country_id\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "# reading from Bronze layer\n",
        "bronze_stream = (spark.readStream\n",
        "                 .schema(message_schema)\n",
        "                 .format(\"parquet\")\n",
        "                 .load(\"content/lake/bronze/messages/data\"))\n",
        "\n",
        "\n",
        "#Function to process each batch\n",
        "def process_batch(df, batch_id):\n",
        "\n",
        "  df_with_country = df.join(countries, on=\"country_id\")\n",
        "\n",
        "  df_with_country = df_with_country.withColumn(\"date\", F.to_date(\"timestamp\"))\n",
        "\n",
        "  df_country_corrupted = df_with_country.filter(\n",
        "        (F.col(\"event_status\").isNull()) |\n",
        "        (F.col(\"event_status\") == \"\") |\n",
        "        (F.col(\"event_status\") == \"NONE\")\n",
        "    )\n",
        "  df_country_non_corrupted = df_with_country.filter(\n",
        "        ~((F.col(\"event_status\").isNull()) |\n",
        "          (F.col(\"event_status\") == \"\") |\n",
        "          (F.col(\"event_status\") == \"NONE\"))\n",
        "    )\n",
        "\n",
        "    # Write corrupted data\n",
        "  (df_country_corrupted.write\n",
        "     .mode(\"append\")\n",
        "     .partitionBy(\"date\")\n",
        "     .format(\"parquet\")\n",
        "     .save(\"content/lake/silver/messages_corrupted/data\"))\n",
        "\n",
        "    # Write not corrupted data\n",
        "  (df_country_non_corrupted.write\n",
        "     .mode(\"append\")\n",
        "     .partitionBy(\"date\")\n",
        "     .format(\"parquet\")\n",
        "     .save(\"content/lake/silver/messages/data\"))\n",
        "\n",
        "\n",
        "\n",
        "query2 = (bronze_stream.writeStream\n",
        "         .outputMode(\"append\")\n",
        "         .foreachBatch(process_batch)\n",
        "         .option(\"checkpointLocation\", \"content/lake/silver/checkpoint\")\n",
        "         .trigger(processingTime=\"5 seconds\")\n",
        "         .start())\n",
        "\n",
        "\n",
        "query2.awaitTermination(20)\n",
        "\n",
        "query2.stop()\n",
        "\n",
        "\n",
        "bronze_df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data\")\n",
        "bronze_count = bronze_df.count()\n",
        "\n",
        "messages_df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data\")\n",
        "silver_messages_count = messages_df.count()\n",
        "\n",
        "corrupted_df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages_corrupted/data\")\n",
        "silver_corrupted_count = corrupted_df.count()\n",
        "\n",
        "total_silver_count = silver_messages_count + silver_corrupted_count\n",
        "\n",
        "if bronze_count == total_silver_count:\n",
        "    print(\"Results match\")\n",
        "else:\n",
        "    print(\"Error, results do not match\")\n",
        "\n",
        "\n",
        "# dedup data\n",
        "\n",
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data\")\n",
        "dedup = df.withColumn(\"row_number\", F.row_number().over(Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\"))).filter(\"row_number = 1\").drop(\"row_number\")\n",
        "\n",
        "\n",
        "# report 1\n",
        "report1 = (\n",
        "    dedup.groupBy(F.col(\"date\"), F.col(\"channel\"))\n",
        "    .pivot(\"event_type\", [\"CLICKED\", \"CREATED\", \"OPEN\", \"RECEIVED\", \"SENT\"])\n",
        "    .count()\n",
        "    .fillna(0)\n",
        ")\n",
        "\n",
        "# Mostrar o relatório 1\n",
        "report1.show()\n",
        "\n",
        "# report 2\n",
        "report2 = (\n",
        "    dedup.groupBy(F.col(\"user_id\"), F.col(\"channel\"))\n",
        "    .count()\n",
        "    .groupBy(\"user_id\")\n",
        "    .pivot(\"channel\", [\"CHAT\", \"EMAIL\", \"OTHER\", \"PUSH\", \"SMS\"])\n",
        "    .sum(\"count\")\n",
        "    .fillna(0)\n",
        ")\n",
        "\n",
        "# Adding iterations column\n",
        "report2 = report2.withColumn(\"iterations\", sum(report2[col] for col in [\"CHAT\", \"EMAIL\", \"OTHER\", \"PUSH\", \"SMS\"]))\n",
        "\n",
        "# Ordering by iteration\n",
        "report2 = report2.orderBy(F.desc(\"iterations\"))\n",
        "\n",
        "report2.show()"
      ],
      "metadata": {
        "id": "QuuLby4iOTe4",
        "outputId": "4eefaf6e-af99-428b-82a5-aafda824da63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|2024-12-13|   PUSH|      2|      0|   2|       0|   0|\n",
            "|2024-12-13|    SMS|      0|      2|   0|       1|   2|\n",
            "|2024-12-13|   CHAT|      3|      1|   3|       2|   2|\n",
            "|2024-12-13|  EMAIL|      0|      2|   1|       3|   1|\n",
            "|2024-12-13|  OTHER|      3|      3|   0|       2|   2|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "\n",
            "+-------+----+-----+-----+----+---+----------+\n",
            "|user_id|CHAT|EMAIL|OTHER|PUSH|SMS|iterations|\n",
            "+-------+----+-----+-----+----+---+----------+\n",
            "|   1021|   1|    1|    1|   0|  0|         3|\n",
            "|   1037|   1|    1|    1|   0|  0|         3|\n",
            "|   1024|   2|    0|    0|   0|  1|         3|\n",
            "|   1009|   1|    0|    0|   1|  1|         3|\n",
            "|   1046|   2|    0|    0|   0|  0|         2|\n",
            "|   1008|   0|    0|    2|   0|  0|         2|\n",
            "|   1047|   0|    0|    1|   1|  0|         2|\n",
            "|   1048|   0|    1|    0|   0|  1|         2|\n",
            "|   1045|   0|    1|    0|   1|  0|         2|\n",
            "|   1039|   0|    0|    0|   0|  2|         2|\n",
            "|   1003|   1|    1|    0|   0|  0|         2|\n",
            "|   1040|   1|    0|    1|   0|  0|         2|\n",
            "|   1030|   0|    0|    1|   0|  0|         1|\n",
            "|   1019|   0|    0|    0|   1|  0|         1|\n",
            "|   1028|   0|    0|    1|   0|  0|         1|\n",
            "|   1010|   0|    0|    0|   0|  1|         1|\n",
            "|   1002|   0|    0|    0|   0|  1|         1|\n",
            "|   1050|   0|    1|    0|   0|  0|         1|\n",
            "|   1017|   0|    1|    0|   0|  0|         1|\n",
            "|   1015|   0|    0|    0|   0|  1|         1|\n",
            "+-------+----+-----+-----+----+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}