{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdias23i/DE-DataProcessing/blob/main/spark/challenges/challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# CHALLENGE 1\n",
        "##  Implement INGESTION process\n",
        "- Set up path in the \"lake\"\n",
        "  - !mkdir -p /content/lake/bronze\n",
        "\n",
        "- Read data from API https://api.carrismetropolitana.pt/\n",
        "  - Endpoints:\n",
        "    - vehicles\n",
        "    - lines\n",
        "    - municipalities\n",
        "  - Use StructFields to enforce schema\n",
        "\n",
        "- Transformations\n",
        "  - vehicles\n",
        "    - create \"date\" extracted from \"timestamp\" column (format: hh24miss)\n",
        "\n",
        "- Write data as PARQUET into the BRONZE layer (/content/lake/bronze)\n",
        "  - Partition \"vehicles\" by \"date\" column\n",
        "  - Paths:\n",
        "    - vehicles - path: /content/lake/bronze/vehicles\n",
        "    - lines - path: /content/lake/bronze/lines\n",
        "    - municipalities - path: /content/lake/bronze/municipalities\n",
        "  - Make sure there is only 1 single parquet created\n",
        "  - Use overwrite as write mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "0bc98aa2-6c1b-4095-e55b-0f89f4a69520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "\n",
        "class ETLFlow:\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def extract_from_api(self, url: str, schema: StructType = None):\n",
        "      response = requests.get(url)\n",
        "      rdd = spark.sparkContext.parallelize(response.json())\n",
        "      if schema:\n",
        "        df = spark.read.schema(schema).json(rdd)\n",
        "      else:\n",
        "        df = spark.read.json(rdd)\n",
        "      return df\n",
        "\n",
        "    def load(self, df: DataFrame, format: str, path: str, **kwargs) -> None:\n",
        "        df.write.mode(\"overwrite\").format(format).save(path)\n",
        "\n",
        "\n",
        "\n",
        "class ETLTask(ETLFlow):\n",
        "\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def ingestion_vehicles(self):\n",
        "      vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                                  StructField('block_id', StringType(), True),\n",
        "                                  StructField('current_status', StringType(), True),\n",
        "                                  StructField('id', StringType(), True),\n",
        "                                  StructField('lat', FloatType(), True),\n",
        "                                  StructField('line_id', StringType(), True),\n",
        "                                  StructField('lon', FloatType(), True),\n",
        "                                  StructField('pattern_id', StringType(), True),\n",
        "                                  StructField('route_id', StringType(), True),\n",
        "                                  StructField('schedule_relationship', StringType(), True),\n",
        "                                  StructField('shift_id', StringType(), True),\n",
        "                                  StructField('speed', FloatType(), True),\n",
        "                                  StructField('stop_id', StringType(), True),\n",
        "                                  StructField('timestamp', TimestampType(), True),\n",
        "                                  StructField('trip_id', StringType(), True)])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/vehicles\", schema = vehicle_schema)\n",
        "      df = df.withColumn('date', to_date(col('timestamp')))\n",
        "\n",
        "      (df\n",
        "        .repartition(1)\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .partitionBy(\"date\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/bronze/vehicles\")\n",
        "      )\n",
        "\n",
        "\n",
        "    def ingestion_lines(self):\n",
        "\n",
        "      lines_schema = StructType([\n",
        "                                    StructField(\"color\", StringType(), True),\n",
        "                                    StructField(\"facilities\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"id\", StringType(), True),\n",
        "                                    StructField(\"localities\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"long_name\", StringType(), True),\n",
        "                                    StructField(\"municipalities\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"patterns\", ArrayType(ArrayType(IntegerType())), True),\n",
        "                                    StructField(\"routes\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"short_name\", StringType(), True),\n",
        "                                    StructField(\"text_color\", StringType(), True)\n",
        "                                ])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/lines\", schema = lines_schema)\n",
        "\n",
        "      (df\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/bronze/lines\")\n",
        "      )\n",
        "\n",
        "\n",
        "    def ingestion_municipalities(self):\n",
        "\n",
        "      municipalities_schema = StructType([\n",
        "                                    StructField(\"district_id\", StringType(), True),\n",
        "                                    StructField(\"district_name\", StringType(), True),\n",
        "                                    StructField(\"id\", StringType(), True),\n",
        "                                    StructField(\"name\", StringType(), True),\n",
        "                                    StructField(\"prefix\", StringType(), True),\n",
        "                                    StructField(\"region_id\", StringType(), True),\n",
        "                                    StructField(\"region_name\", StringType(), True)\n",
        "                                ])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/municipalities\", schema = municipalities_schema)\n",
        "\n",
        "      (df\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/bronze/municipalities\")\n",
        "      )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master('local').appName('Challenge 1').getOrCreate()\n",
        "\n",
        "    print(\"Starting Challenging 1\")\n",
        "    etl = ETLTask(spark)\n",
        "\n",
        "    # run tasks\n",
        "    print(\"Running Task - Ingestion Vehicles\")\n",
        "    etl.ingestion_vehicles()\n",
        "\n",
        "    print(\"Running Task - Ingestion Lines\")\n",
        "    etl.ingestion_lines()\n",
        "\n",
        "    print(\"Running Task - Ingestion municipalities\")\n",
        "    etl.ingestion_municipalities()"
      ],
      "metadata": {
        "id": "45wNLEbnVCfp",
        "outputId": "4a90fcb1-9c65-4475-ec02-eb7c943e1594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Challenging 1\n",
            "Running Task - Ingestion Vehicles\n",
            "Running Task - Ingestion Lines\n",
            "Running Task - Ingestion municipalities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lCAIUYcU4gSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "\n",
        "# Path to files\n",
        "parquet_vehicles_file = \"/content/lake/bronze/vehicles\"\n",
        "parquet_lines_file = \"/content/lake/bronze/lines\"\n",
        "parquet_municipalities_file = \"/content/lake/bronze/municipalities\"\n",
        "\n",
        "\n",
        "\n",
        "class ETLFlow:\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def load(self, df: DataFrame, format: str, path: str, **kwargs) -> None:\n",
        "        df.write.mode(\"overwrite\").format(format).save(path)\n",
        "\n",
        "class ETLTask(ETLFlow):\n",
        "\n",
        "  def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "  def cleansing_vehicles(self):\n",
        "    #Read vehicles parquet file\n",
        "    df_vehicles = spark.read.parquet(parquet_vehicles_file)\n",
        "\n",
        "\n",
        "    # Transformations\n",
        "    # rename \"lat\" and \"lon\" to \"latitude\" and \"longitude\" respectively\n",
        "    df_vehicles = df_vehicles.withColumnRenamed(\"lat\", \"latitude\")\n",
        "    df_vehicles = df_vehicles.withColumnRenamed(\"lon\", \"longitude\")\n",
        "    # remove possible duplicates\n",
        "    df_vehicles = df_vehicles.drop_duplicates()\n",
        "    # remove rows when the column CURRENT_STATUS is null\n",
        "    df_vehicles = df_vehicles.filter(df_vehicles['current_status'].isNotNull())\n",
        "\n",
        "\n",
        "    (df_vehicles\n",
        "        .repartition(1)\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .partitionBy(\"date\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/silver/vehicles\")\n",
        "      )\n",
        "\n",
        "\n",
        "  def cleansing_lines(self):\n",
        "    #Read lines parquet file\n",
        "    df_lines = spark.read.parquet(parquet_lines_file)\n",
        "    df_lines.show()\n",
        "    # remove possible duplicates\n",
        "    df_lines = df_lines.drop_duplicates()\n",
        "    #remove rows when the column LONG_NAME is null\n",
        "    df_lines = df_lines.filter(df_lines['long_name'].isNotNull())\n",
        "\n",
        "    self.load(df=df_lines, format=\"parquet\", path=\"/content/lake/silver/lines\")\n",
        "\n",
        "\n",
        "\n",
        "  def cleansing_municipalities(self):\n",
        "    #Read municipalities parquet file\n",
        "    df_municipalities = spark.read.parquet(parquet_municipalities_file)\n",
        "    # remove possible duplicates\n",
        "    df_municipalities = df_municipalities.drop_duplicates()\n",
        "    #remove rows when the columns NAME or DISTRICT_NAME are null\n",
        "    df_municipalities = df_municipalities.filter((df_municipalities['name'].isNotNull()) & (col(\"district_name\").isNotNull()))\n",
        "\n",
        "    self.load(df=df_municipalities, format=\"parquet\", path=\"/content/lake/silver/municipalities\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master('local').appName('Challenge 2').getOrCreate()\n",
        "\n",
        "    print(\"Starting Challenging 2\")\n",
        "    etl = ETLTask(spark)\n",
        "\n",
        "    # run tasks\n",
        "    print(\"Running Task - Cleansing Vehicles\")\n",
        "    etl.cleansing_vehicles()\n",
        "    print(\"Running Task - Cleansing Lines\")\n",
        "    etl.cleansing_lines()\n",
        "    print(\"Running Task - Cleansing Municipalities\")\n",
        "    etl.cleansing_municipalities()"
      ],
      "metadata": {
        "id": "-kNQd1Edj55d",
        "outputId": "bc771de3-7938-4953-a9f9-4250337ad8c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Challenging 2\n",
            "Running Task - Cleansing Vehicles\n",
            "Running Task - Cleansing Lines\n",
            "Running Task - Cleansing Municipalities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadParquetExample\").getOrCreate()\n",
        "\n",
        "# Path to files\n",
        "parquet_vehicles_file = \"/content/lake/silver/vehicles\"\n",
        "parquet_lines_file = \"/content/lake/silver/lines\"\n",
        "parquet_municipalities_file = \"/content/lake/silver/municipalities\"\n",
        "\n",
        "class ETLTask(ETLFlow):\n",
        "  def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "  def enrich_vehicles(self):\n",
        "\n",
        "    #Read parquet files\n",
        "    df_vehicles = spark.read.parquet(parquet_vehicles_file)\n",
        "    df_lines = spark.read.parquet(parquet_lines_file)\n",
        "    df_municipalities = spark.read.parquet(parquet_municipalities_file)\n",
        "\n",
        "\n",
        "    df_lines = df_lines.withColumn('municipality_id', explode(df_lines['municipalities']))\n",
        "\n",
        "    df_vehicles.createOrReplaceTempView(\"vehicles\")\n",
        "    df_lines.createOrReplaceTempView(\"lines\")\n",
        "    df_municipalities.createOrReplaceTempView(\"municipalities\")\n",
        "\n",
        "\n",
        "    query = \"\"\"\n",
        "              SELECT\n",
        "                  vehicles.*,  -- all columns vehicles\n",
        "                  lines.long_name AS line_name,  --  lines.long_name to line_name\n",
        "                  municipalities.name AS municipality_name  --  municipalities.name to municipality_name\n",
        "              FROM\n",
        "                  vehicles\n",
        "              JOIN\n",
        "                  lines ON vehicles.line_id = lines.id  -- Join vehicles and lines\n",
        "              JOIN\n",
        "                  municipalities ON lines.municipality_id = municipalities.id  -- Join municipalities\n",
        "            \"\"\"\n",
        "\n",
        "    result_df = spark.sql(query)\n",
        "\n",
        "    (result_df\n",
        "        .repartition(1)\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .partitionBy(\"date\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/gold/vehicles_enriched\")\n",
        "      )\n",
        "\n",
        "    result_df.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master('local').appName('Challenge 3').getOrCreate()\n",
        "\n",
        "    print(\"Starting Challenging 3\")\n",
        "    etl = ETLTask(spark)\n",
        "\n",
        "    # run tasks\n",
        "    print(\"Running Task - Enrich Vehicles\")\n",
        "    etl.enrich_vehicles()\n",
        ""
      ],
      "metadata": {
        "id": "ek4Uosa8kxzJ",
        "outputId": "fc0e4bb6-7d6a-417e-caac-edf17aa75d02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Challenging 3\n",
            "Running Task - Enrich Vehicles\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+--------------------+-------------------+\n",
            "|bearing|            block_id|current_status|      id| latitude|line_id|longitude|pattern_id|route_id|schedule_relationship|    shift_id|    speed|stop_id|          timestamp|             trip_id|      date|           line_name|  municipality_name|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+--------------------+-------------------+\n",
            "|     64|             1185-11| IN_TRANSIT_TO| 42|2369|38.907845|   2328|-9.029838|  2328_0_1|  2328_0|            SCHEDULED|        1279|11.111111| 180095|2024-11-22 21:30:28|2328_0_1|1|1|2100...|2024-11-22|QªPied - Vila Fra...|Vila Franca de Xira|\n",
            "|    268|           1_1082-11| IN_TRANSIT_TO| 41|1319| 38.76858|   1014| -9.24558|  1014_0_1|  1014_0|            SCHEDULED|   1103+1930|3.3333333| 030129|2024-11-22 21:30:33|1014_0_1_2100_212...|2024-11-22|Amadora (Cemitéri...|            Amadora|\n",
            "|    170|       ESC_DU_EU1027| IN_TRANSIT_TO| 43|2209|38.655785|   3715|-9.154193|  3715_0_1|  3715_0|            SCHEDULED|      EU1135|5.5555553| 020859|2024-11-22 21:30:15|3715_0_1_2100_212...|2024-11-22|Lisboa (M. Pombal...|             Seixal|\n",
            "|    170|       ESC_DU_EU1027| IN_TRANSIT_TO| 43|2209|38.655785|   3715|-9.154193|  3715_0_1|  3715_0|            SCHEDULED|      EU1135|5.5555553| 020859|2024-11-22 21:30:15|3715_0_1_2100_212...|2024-11-22|Lisboa (M. Pombal...|             Almada|\n",
            "|    170|       ESC_DU_EU1027| IN_TRANSIT_TO| 43|2209|38.655785|   3715|-9.154193|  3715_0_1|  3715_0|            SCHEDULED|      EU1135|5.5555553| 020859|2024-11-22 21:30:15|3715_0_1_2100_212...|2024-11-22|Lisboa (M. Pombal...|             Lisboa|\n",
            "|    129|20241122-64010254...| IN_TRANSIT_TO|44|12573| 38.58558|   4632|-9.052158|  4632_0_1|  4632_0|            SCHEDULED|111630234560|      0.0| 140028|2024-11-22 21:30:37|4632_0_1|2200|213...|2024-11-22|Coina (Estação) -...|           Barreiro|\n",
            "|    129|20241122-64010254...| IN_TRANSIT_TO|44|12573| 38.58558|   4632|-9.052158|  4632_0_1|  4632_0|            SCHEDULED|111630234560|      0.0| 140028|2024-11-22 21:30:37|4632_0_1|2200|213...|2024-11-22|Coina (Estação) -...|           Sesimbra|\n",
            "|    129|20241122-64010254...| IN_TRANSIT_TO|44|12573| 38.58558|   4632|-9.052158|  4632_0_1|  4632_0|            SCHEDULED|111630234560|      0.0| 140028|2024-11-22 21:30:37|4632_0_1|2200|213...|2024-11-22|Coina (Estação) -...|            Setúbal|\n",
            "|    129|20241122-64010254...| IN_TRANSIT_TO|44|12573| 38.58558|   4632|-9.052158|  4632_0_1|  4632_0|            SCHEDULED|111630234560|      0.0| 140028|2024-11-22 21:30:37|4632_0_1|2200|213...|2024-11-22|Coina (Estação) -...|            Palmela|\n",
            "|    178|       ESC_DU_EU2018|    STOPPED_AT| 43|2082|38.655346|   3034|-9.201068|  3034_0_1|  3034_0|            SCHEDULED|      EU2201|1.1111112| 020319|2024-11-22 21:30:40|3034_0_1_2100_212...|2024-11-22|Porto Brandão (Te...|             Almada|\n",
            "|    343|             1530-11| IN_TRANSIT_TO| 42|2539|38.794758|   2203|-9.195343|  2203_0_3|  2203_0|            SCHEDULED|        1659|11.388889| 110071|2024-11-22 21:30:41|2203_0_3|1|1|2105...|2024-11-22|Arroja | Circular...|           Odivelas|\n",
            "|    162|UNAVAILABLE_BLOCK_ID|    STOPPED_AT| 41|1372|38.757507|   1518|-9.225989|  1518_0_1|  1518_0|            SCHEDULED|        1694|13.055555| 030605|2024-11-22 21:30:39|1518_0_1_2100_212...|2024-11-22|Monte Abraão - Re...|            Amadora|\n",
            "|    162|UNAVAILABLE_BLOCK_ID|    STOPPED_AT| 41|1372|38.757507|   1518|-9.225989|  1518_0_1|  1518_0|            SCHEDULED|        1694|13.055555| 030605|2024-11-22 21:30:39|1518_0_1_2100_212...|2024-11-22|Monte Abraão - Re...|             Sintra|\n",
            "|    223|           1_1205-11|    STOPPED_AT| 41|1172|38.766476|   1715|-9.297747|  1715_0_1|  1715_0|            SCHEDULED|        1257|7.2222223| 170459|2024-11-22 21:30:05|1715_0_1_2030_205...|2024-11-22|Belém (Estação) -...|             Sintra|\n",
            "|    223|           1_1205-11|    STOPPED_AT| 41|1172|38.766476|   1715|-9.297747|  1715_0_1|  1715_0|            SCHEDULED|        1257|7.2222223| 170459|2024-11-22 21:30:05|1715_0_1_2030_205...|2024-11-22|Belém (Estação) -...|            Amadora|\n",
            "|    223|           1_1205-11|    STOPPED_AT| 41|1172|38.766476|   1715|-9.297747|  1715_0_1|  1715_0|            SCHEDULED|        1257|7.2222223| 170459|2024-11-22 21:30:05|1715_0_1_2030_205...|2024-11-22|Belém (Estação) -...|             Oeiras|\n",
            "|    223|           1_1205-11|    STOPPED_AT| 41|1172|38.766476|   1715|-9.297747|  1715_0_1|  1715_0|            SCHEDULED|        1257|7.2222223| 170459|2024-11-22 21:30:05|1715_0_1_2030_205...|2024-11-22|Belém (Estação) -...|             Lisboa|\n",
            "|    159|           1_1304-11| IN_TRANSIT_TO| 41|1398|38.691654|   1638|-9.359159|  1638_0_2|  1638_0|            SCHEDULED|        1365|6.9444447| 050047|2024-11-22 21:30:20|1638_0_2_2130_215...|2024-11-22|Oeiras (Estação N...|            Cascais|\n",
            "|    159|           1_1304-11| IN_TRANSIT_TO| 41|1398|38.691654|   1638|-9.359159|  1638_0_2|  1638_0|            SCHEDULED|        1365|6.9444447| 050047|2024-11-22 21:30:20|1638_0_2_2130_215...|2024-11-22|Oeiras (Estação N...|             Oeiras|\n",
            "|    297|             1038-11| IN_TRANSIT_TO| 42|2325| 38.80507|   2736|-9.112374|  2736_0_3|  2736_0|            SCHEDULED|        1261|14.166667| 070111|2024-11-22 21:30:11|2736_0_3|1|1|2100...|2024-11-22|Campo Grande - Ca...|             Loures|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadParquetExample\").getOrCreate()\n",
        "\n",
        "# Path to files\n",
        "parquet_vehicles_enriched = \"/content/lake/gold/vehicles_enriched\"\n",
        "\n",
        "\n",
        "class ETLTask():\n",
        "  def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "  def gold_vehicles(self):\n",
        "\n",
        "    #Read parquet files\n",
        "    df_vehicles_enriched = spark.read.parquet(parquet_vehicles_enriched)\n",
        "\n",
        "    df_vehicles_enriched_by_municipality = df_vehicles_enriched.groupBy(\"municipality_name\").agg(\n",
        "      count(\"line_id\").alias(\"vehicle_count\"),\n",
        "      sum(\"speed\").alias(\"total_speed\")\n",
        "    ).orderBy(desc(col(\"total_speed\"))).limit(3)\n",
        "\n",
        "    df_vehicles_enriched_by_municipality_avg = df_vehicles_enriched.groupBy(\"municipality_name\").agg(\n",
        "      avg(\"speed\").alias(\"avg_speed\")\n",
        "    ).orderBy(desc(col(\"avg_speed\"))).limit(3)\n",
        "\n",
        "    df_vehicles_enriched_by_municipality.show()\n",
        "\n",
        "    df_vehicles_enriched_by_municipality_avg.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master('local').appName('Challenge 4').getOrCreate()\n",
        "\n",
        "    print(\"Starting Challenging 4\")\n",
        "    etl = ETLTask(spark)\n",
        "\n",
        "     # run tasks\n",
        "    print(\"Running Task\")\n",
        "    etl.gold_vehicles()\n",
        ""
      ],
      "metadata": {
        "id": "i9DTJMjyymOG",
        "outputId": "c04d8635-4f35-4495-ca0d-fc0dab36e9d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Challenging 4\n",
            "Running Task\n",
            "+-----------------+-------------+-----------------+\n",
            "|municipality_name|vehicle_count|      total_speed|\n",
            "+-----------------+-------------+-----------------+\n",
            "|           Lisboa|          157|1092.499995470047|\n",
            "|           Loures|           87|521.6666631698608|\n",
            "|           Sintra|           83|466.1111090183258|\n",
            "+-----------------+-------------+-----------------+\n",
            "\n",
            "+-----------------+-----------------+\n",
            "|municipality_name|        avg_speed|\n",
            "+-----------------+-----------------+\n",
            "|         Alenquer| 9.44444465637207|\n",
            "|            Mafra|9.398148112826878|\n",
            "|          Montijo|9.355158720697675|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}