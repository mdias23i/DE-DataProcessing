{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdias23i/DE-DataProcessing/blob/main/spark/challenges/challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# CHALLENGE 1\n",
        "##  Implement INGESTION process\n",
        "- Set up path in the \"lake\"\n",
        "  - !mkdir -p /content/lake/bronze\n",
        "\n",
        "- Read data from API https://api.carrismetropolitana.pt/\n",
        "  - Endpoints:\n",
        "    - vehicles\n",
        "    - lines\n",
        "    - municipalities\n",
        "  - Use StructFields to enforce schema\n",
        "\n",
        "- Transformations\n",
        "  - vehicles\n",
        "    - create \"date\" extracted from \"timestamp\" column (format: hh24miss)\n",
        "\n",
        "- Write data as PARQUET into the BRONZE layer (/content/lake/bronze)\n",
        "  - Partition \"vehicles\" by \"date\" column\n",
        "  - Paths:\n",
        "    - vehicles - path: /content/lake/bronze/vehicles\n",
        "    - lines - path: /content/lake/bronze/lines\n",
        "    - municipalities - path: /content/lake/bronze/municipalities\n",
        "  - Make sure there is only 1 single parquet created\n",
        "  - Use overwrite as write mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "0bc98aa2-6c1b-4095-e55b-0f89f4a69520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "\n",
        "class ETLFlow:\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def extract_from_api(self, url: str, schema: StructType = None):\n",
        "      response = requests.get(url)\n",
        "      rdd = spark.sparkContext.parallelize(response.json())\n",
        "      if schema:\n",
        "        df = spark.read.schema(schema).json(rdd)\n",
        "      else:\n",
        "        df = spark.read.json(rdd)\n",
        "      return df\n",
        "\n",
        "    def load(self, df: DataFrame, format: str, path: str, **kwargs) -> None:\n",
        "        df.write.mode(\"overwrite\").format(format).save(path)\n",
        "\n",
        "\n",
        "\n",
        "class ETLTask(ETLFlow):\n",
        "\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def ingestion_vehicles(self):\n",
        "      vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                                  StructField('block_id', StringType(), True),\n",
        "                                  StructField('current_status', StringType(), True),\n",
        "                                  StructField('id', StringType(), True),\n",
        "                                  StructField('lat', FloatType(), True),\n",
        "                                  StructField('line_id', StringType(), True),\n",
        "                                  StructField('lon', FloatType(), True),\n",
        "                                  StructField('pattern_id', StringType(), True),\n",
        "                                  StructField('route_id', StringType(), True),\n",
        "                                  StructField('schedule_relationship', StringType(), True),\n",
        "                                  StructField('shift_id', StringType(), True),\n",
        "                                  StructField('speed', FloatType(), True),\n",
        "                                  StructField('stop_id', StringType(), True),\n",
        "                                  StructField('timestamp', TimestampType(), True),\n",
        "                                  StructField('trip_id', StringType(), True)])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/vehicles\", schema = vehicle_schema)\n",
        "      df = df.withColumn('date', to_date(col('timestamp')))\n",
        "\n",
        "      (df\n",
        "        .repartition(1)\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .partitionBy(\"date\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/bronze/vehicles\")\n",
        "      )\n",
        "\n",
        "\n",
        "    def ingestion_lines(self):\n",
        "\n",
        "      lines_schema = StructType([\n",
        "                                    StructField(\"color\", StringType(), True),\n",
        "                                    StructField(\"facilities\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"id\", StringType(), True),\n",
        "                                    StructField(\"localities\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"long_name\", StringType(), True),\n",
        "                                    StructField(\"municipalities\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"patterns\", ArrayType(ArrayType(IntegerType())), True),\n",
        "                                    StructField(\"routes\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"short_name\", StringType(), True),\n",
        "                                    StructField(\"text_color\", StringType(), True)\n",
        "                                ])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/lines\", schema = lines_schema)\n",
        "\n",
        "      (df\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/bronze/lines\")\n",
        "      )\n",
        "\n",
        "\n",
        "    def ingestion_municipalities(self):\n",
        "\n",
        "      municipalities_schema = StructType([\n",
        "                                    StructField(\"district_id\", StringType(), True),\n",
        "                                    StructField(\"district_name\", StringType(), True),\n",
        "                                    StructField(\"id\", StringType(), True),\n",
        "                                    StructField(\"name\", StringType(), True),\n",
        "                                    StructField(\"prefix\", StringType(), True),\n",
        "                                    StructField(\"region_id\", StringType(), True),\n",
        "                                    StructField(\"region_name\", StringType(), True)\n",
        "                                ])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/municipalities\", schema = municipalities_schema)\n",
        "\n",
        "      (df\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/bronze/municipalities\")\n",
        "      )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master('local').appName('Challenge 1').getOrCreate()\n",
        "\n",
        "    print(\"Starting Challenging 1\")\n",
        "    etl = ETLTask(spark)\n",
        "\n",
        "    # run tasks\n",
        "    print(\"Running Task - Ingestion Vehicles\")\n",
        "    etl.ingestion_vehicles()\n",
        "\n",
        "    print(\"Running Task - Ingestion Lines\")\n",
        "    etl.ingestion_lines()\n",
        "\n",
        "    print(\"Running Task - Ingestion municipalities\")\n",
        "    etl.ingestion_municipalities()"
      ],
      "metadata": {
        "id": "45wNLEbnVCfp",
        "outputId": "535d1444-f1ad-4ef7-dd1d-4e3a30b7bf5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Challenging 1\n",
            "Running Task - Ingestion Vehicles\n",
            "Running Task - Ingestion Lines\n",
            "Running Task - Ingestion municipalities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lCAIUYcU4gSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "\n",
        "# Path to files\n",
        "parquet_vehicles_file = \"/content/lake/bronze/vehicles\"\n",
        "parquet_lines_file = \"/content/lake/bronze/lines\"\n",
        "parquet_municipalities_file = \"/content/lake/bronze/municipalities\"\n",
        "\n",
        "\n",
        "\n",
        "class ETLFlow:\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def load(self, df: DataFrame, format: str, path: str, **kwargs) -> None:\n",
        "        df.write.mode(\"overwrite\").format(format).save(path)\n",
        "\n",
        "class ETLTask(ETLFlow):\n",
        "\n",
        "  def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "  def cleansing_vehicles(self):\n",
        "    #Read vehicles parquet file\n",
        "    df_vehicles = spark.read.parquet(parquet_vehicles_file)\n",
        "\n",
        "\n",
        "    # Transformations\n",
        "    # rename \"lat\" and \"lon\" to \"latitude\" and \"longitude\" respectively\n",
        "    df_vehicles = df_vehicles.withColumnRenamed(\"lat\", \"latitude\")\n",
        "    df_vehicles = df_vehicles.withColumnRenamed(\"lon\", \"longitude\")\n",
        "    # remove possible duplicates\n",
        "    df_vehicles = df_vehicles.drop_duplicates()\n",
        "    # remove rows when the column CURRENT_STATUS is null\n",
        "    df_vehicles = df_vehicles.filter(df_vehicles['current_status'].isNotNull())\n",
        "\n",
        "\n",
        "    (df_vehicles\n",
        "        .repartition(1)\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .partitionBy(\"date\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/silver/vehicles\")\n",
        "      )\n",
        "\n",
        "\n",
        "  def cleansing_lines(self):\n",
        "    #Read lines parquet file\n",
        "    df_lines = spark.read.parquet(parquet_lines_file)\n",
        "    # remove possible duplicates\n",
        "    df_lines = df_lines.drop_duplicates()\n",
        "    #remove rows when the column LONG_NAME is null\n",
        "    df_lines = df_lines.filter(df_lines['long_name'].isNotNull())\n",
        "\n",
        "    self.load(df=df_lines, format=\"parquet\", path=\"/content/lake/silver/lines\")\n",
        "\n",
        "\n",
        "\n",
        "  def cleansing_municipalities(self):\n",
        "    #Read municipalities parquet file\n",
        "    df_municipalities = spark.read.parquet(parquet_municipalities_file)\n",
        "    # remove possible duplicates\n",
        "    df_municipalities = df_municipalities.drop_duplicates()\n",
        "    #remove rows when the columns NAME or DISTRICT_NAME are null\n",
        "    df_municipalities = df_municipalities.filter((df_municipalities['name'].isNotNull()) & (col(\"district_name\").isNotNull()))\n",
        "\n",
        "    self.load(df=df_municipalities, format=\"parquet\", path=\"/content/lake/silver/municipalities\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master('local').appName('Challenge 2').getOrCreate()\n",
        "\n",
        "    print(\"Starting Challenging 2\")\n",
        "    etl = ETLTask(spark)\n",
        "\n",
        "    # run tasks\n",
        "    print(\"Running Task - Cleansing Vehicles\")\n",
        "    etl.cleansing_vehicles()\n",
        "    print(\"Running Task - Cleansing Lines\")\n",
        "    etl.cleansing_lines()\n",
        "    print(\"Running Task - Cleansing Municipalities\")\n",
        "    etl.cleansing_municipalities()"
      ],
      "metadata": {
        "id": "-kNQd1Edj55d",
        "outputId": "650981a8-ee32-41e7-dec7-e1ea5e388105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Challenging 2\n",
            "Running Task - Cleansing Vehicles\n",
            "Running Task - Cleansing Lines\n",
            "+-------+----------+----+--------------------+--------------------+--------------------+--------+--------------------+----------+----------+\n",
            "|  color|facilities|  id|          localities|           long_name|      municipalities|patterns|              routes|short_name|text_color|\n",
            "+-------+----------+----+--------------------+--------------------+--------------------+--------+--------------------+----------+----------+\n",
            "|#FDB71A|        []|2740|[Mafra, Antas, Al...|Ericeira (Termina...|        [1109, 1106]|    NULL|[2740_0, 2740_1, ...|      2740|   #FFFFFF|\n",
            "|#FDB71A|        []|2741|[Mafra, Antas, Al...|Ericeira (Termina...|  [1109, 1107, 1106]|    NULL|    [2741_0, 2741_1]|      2741|   #FFFFFF|\n",
            "|#FDB71A|        []|2742|[Campo Grande, Lo...|Lisboa (C. Grande...|  [1106, 1107, 1109]|    NULL|            [2742_0]|      2742|   #FFFFFF|\n",
            "|#C61D23|        []|2743|[Campo Grande, Lo...|Lisboa (C. Grande...|  [1106, 1107, 1109]|    NULL|            [2743_0]|      2743|   #FFFFFF|\n",
            "|#FDB71A|        []|2744|[Campo Grande, Lo...|Lisboa (C. Grande...|  [1106, 1107, 1109]|    NULL|            [2744_0]|      2744|   #FFFFFF|\n",
            "|#C61D23|        []|2745|[Mafra, Loures, C...|Póvoa da Galega -...|  [1109, 1107, 1106]|    NULL|    [2745_0, 2745_1]|      2745|   #FFFFFF|\n",
            "|#FDB71A|        []|2746|[Campo Grande, Lo...|Lisboa (C. Grande...|  [1106, 1107, 1109]|    NULL|            [2746_0]|      2746|   #FFFFFF|\n",
            "|#C61D23|        []|2750|[Loures, Freixial...|Bucelas - Lisboa ...|[1107, 1109, 1116...|    NULL|            [2750_0]|      2750|   #FFFFFF|\n",
            "|#C61D23|        []|2751|[Mafra, Antas, Al...|Ericeira (Termina...|[1109, 1107, 1116...|    NULL|    [2751_0, 2751_1]|      2751|   #FFFFFF|\n",
            "|#C61D23|        []|2752|[Campo Grande, Od...|Lisboa (C. Grande...|[1106, 1116, 1107...|    NULL|            [2752_0]|      2752|   #FFFFFF|\n",
            "|#C61D23|        []|2753|[Campo Grande, Od...|Lisboa (C. Grande...|[1106, 1116, 1107...|    NULL|            [2753_0]|      2753|   #FFFFFF|\n",
            "|#C61D23|        []|2754|[Campo Grande, Od...|Lisboa (C. Grande...|[1106, 1116, 1107...|    NULL|[2754_0, 2754_1, ...|      2754|   #FFFFFF|\n",
            "|#C61D23|        []|2755|[Campo Grande, Od...|Lisboa (C. Grande...|[1106, 1116, 1107...|    NULL|            [2755_0]|      2755|   #FFFFFF|\n",
            "|#C61D23|        []|2756|[Campo Grande, Od...|Lisboa (C. Grande...|[1106, 1116, 1107...|    NULL|    [2756_0, 2756_1]|      2756|   #FFFFFF|\n",
            "|#C61D23|        []|2758|[Campo Grande, Od...|Lisboa (C. Grande...|[1106, 1116, 1107...|    NULL|            [2758_0]|      2758|   #FFFFFF|\n",
            "|#C61D23|        []|2761|[Loures, Freixial...|Bucelas - Lisboa ...|[1107, 1109, 1116...|    NULL|            [2761_0]|      2761|   #FFFFFF|\n",
            "|#C61D23|        []|2762|[Loures, Ribas de...|Bucelas - Lisboa ...|        [1107, 1106]|    NULL|            [2762_0]|      2762|   #FFFFFF|\n",
            "|#C61D23|        []|2764|[Loures, Bucelas,...|Bucelas - Lisboa ...|  [1107, 1116, 1106]|    NULL|            [2764_0]|      2764|   #FFFFFF|\n",
            "|#C61D23|        []|2765|[Loures, Sto. Ant...|Cabeço de Montach...|[1107, 1109, 1116...|    NULL|    [2765_0, 2765_1]|      2765|   #FFFFFF|\n",
            "|#C61D23|        []|2768|[Loures, Santo An...|Casal Paradela - ...|  [1107, 1116, 1106]|    NULL|            [2768_0]|      2768|   #FFFFFF|\n",
            "+-------+----------+----+--------------------+--------------------+--------------------+--------+--------------------+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Running Task - Cleansing Municipalities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadParquetExample\").getOrCreate()\n",
        "\n",
        "# Path to files\n",
        "parquet_vehicles_file = \"/content/lake/silver/vehicles\"\n",
        "parquet_lines_file = \"/content/lake/silver/lines\"\n",
        "parquet_municipalities_file = \"/content/lake/silver/municipalities\"\n",
        "\n",
        "class ETLTask(ETLFlow):\n",
        "  def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "  def enrich_vehicles(self):\n",
        "\n",
        "    #Read parquet files\n",
        "    df_vehicles = spark.read.parquet(parquet_vehicles_file)\n",
        "    df_lines = spark.read.parquet(parquet_lines_file)\n",
        "    df_municipalities = spark.read.parquet(parquet_municipalities_file)\n",
        "\n",
        "\n",
        "    df_lines = df_lines.withColumn('municipality_id', explode(df_lines['municipalities']))\n",
        "\n",
        "    df_vehicles.createOrReplaceTempView(\"vehicles\")\n",
        "    df_lines.createOrReplaceTempView(\"lines\")\n",
        "    df_municipalities.createOrReplaceTempView(\"municipalities\")\n",
        "\n",
        "\n",
        "    query = \"\"\"\n",
        "              SELECT\n",
        "                  vehicles.*,  -- all columns vehicles\n",
        "                  lines.long_name AS line_name,  --  lines.long_name to line_name\n",
        "                  municipalities.name AS municipality_name  --  municipalities.name to municipality_name\n",
        "              FROM\n",
        "                  vehicles\n",
        "              JOIN\n",
        "                  lines ON vehicles.line_id = lines.id  -- Join vehicles and lines\n",
        "              JOIN\n",
        "                  municipalities ON lines.municipality_id = municipalities.id  -- Join municipalities\n",
        "            \"\"\"\n",
        "\n",
        "    result_df = spark.sql(query)\n",
        "\n",
        "    (result_df\n",
        "        .repartition(1)\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .partitionBy(\"date\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/gold/vehicles_enriched\")\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master('local').appName('Challenge 3').getOrCreate()\n",
        "\n",
        "    print(\"Starting Challenging 3\")\n",
        "    etl = ETLTask(spark)\n",
        "\n",
        "    # run tasks\n",
        "    print(\"Running Task - Enrich Vehicles\")\n",
        "    etl.enrich_vehicles()\n"
      ],
      "metadata": {
        "id": "ek4Uosa8kxzJ",
        "outputId": "8314f28a-5470-4f63-bcae-60b22abb48c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Challenging 3\n",
            "Running Task - Enrich Vehicles\n",
            "+-------+-------------+--------------+-------+---------+-------+---------+----------+--------+---------------------+---------+---------+-------+-------------------+--------------------+----------+--------------------+-------------------+\n",
            "|bearing|     block_id|current_status|     id| latitude|line_id|longitude|pattern_id|route_id|schedule_relationship| shift_id|    speed|stop_id|          timestamp|             trip_id|      date|           line_name|  municipality_name|\n",
            "+-------+-------------+--------------+-------+---------+-------+---------+----------+--------+---------------------+---------+---------+-------+-------------------+--------------------+----------+--------------------+-------------------+\n",
            "|     31|      1070-11|    STOPPED_AT|42|2359| 38.76794|   2711|-9.100332|  2711_0_2|  2711_0|            SCHEDULED|     1207|      0.0| 060322|2024-11-22 22:27:37|2711_0_2|1|1|2230...|2024-11-22|Charneca - Est Or...|             Loures|\n",
            "|     31|      1070-11|    STOPPED_AT|42|2359| 38.76794|   2711|-9.100332|  2711_0_2|  2711_0|            SCHEDULED|     1207|      0.0| 060322|2024-11-22 22:27:37|2711_0_2|1|1|2230...|2024-11-22|Charneca - Est Or...|             Lisboa|\n",
            "|    360|      1191-11| IN_TRANSIT_TO|42|2014|38.947502|   2309| -9.01925|  2309_0_1|  2309_0|            SCHEDULED|     1282| 9.444445| 180267|2024-11-22 22:27:55|2309_0_1|1|1|2200...|2024-11-22|Bogalhão - Vila F...|Vila Franca de Xira|\n",
            "|     80|ESC_DU_EU2032|    STOPPED_AT|43|2319| 38.66768|   3507|-9.188401|  3507_0_2|  3507_0|            SCHEDULED|   EU2232|0.2777778| 020255|2024-11-22 22:28:00|3507_0_2_2200_222...|2024-11-22|Cacilhas (Termina...|             Seixal|\n",
            "|     80|ESC_DU_EU2032|    STOPPED_AT|43|2319| 38.66768|   3507|-9.188401|  3507_0_2|  3507_0|            SCHEDULED|   EU2232|0.2777778| 020255|2024-11-22 22:28:00|3507_0_2_2200_222...|2024-11-22|Cacilhas (Termina...|             Almada|\n",
            "|    165|      1037-11|    STOPPED_AT|42|2100|38.824093|   2725|-9.137364|  2725_0_2|  2725_0|            SCHEDULED|     1245| 9.166667| 070498|2024-11-22 22:27:47|2725_0_2|1|1|2210...|2024-11-22|Estação Oriente -...|             Loures|\n",
            "|    165|      1037-11|    STOPPED_AT|42|2100|38.824093|   2725|-9.137364|  2725_0_2|  2725_0|            SCHEDULED|     1245| 9.166667| 070498|2024-11-22 22:27:47|2725_0_2|1|1|2210...|2024-11-22|Estação Oriente -...|             Lisboa|\n",
            "|    358| 1039-11'_001| IN_TRANSIT_TO|42|2350|38.784008|   2713|-9.142814|  2713_0_3|  2713_0|            SCHEDULED|1208'_001| 9.166667| 071378|2024-11-22 22:27:37|2713_0_3|1|1|2215...|2024-11-22|Campo Grande - Ca...|             Loures|\n",
            "|    358| 1039-11'_001| IN_TRANSIT_TO|42|2350|38.784008|   2713|-9.142814|  2713_0_3|  2713_0|            SCHEDULED|1208'_001| 9.166667| 071378|2024-11-22 22:27:37|2713_0_3|1|1|2215...|2024-11-22|Campo Grande - Ca...|             Lisboa|\n",
            "|      0|      1507-11|    STOPPED_AT|42|2566|38.766045|   2605|-9.297876|  2605_0_1|  2605_0|            SCHEDULED|     1635|      0.0| 172600|2024-11-22 22:27:46|2605_0_1|1|1|2230...|2024-11-22|Cacém (Estação) -...|             Loures|\n",
            "|      0|      1507-11|    STOPPED_AT|42|2566|38.766045|   2605|-9.297876|  2605_0_1|  2605_0|            SCHEDULED|     1635|      0.0| 172600|2024-11-22 22:27:46|2605_0_1|1|1|2230...|2024-11-22|Cacém (Estação) -...|           Odivelas|\n",
            "|      0|      1507-11|    STOPPED_AT|42|2566|38.766045|   2605|-9.297876|  2605_0_1|  2605_0|            SCHEDULED|     1635|      0.0| 172600|2024-11-22 22:27:46|2605_0_1|1|1|2230...|2024-11-22|Cacém (Estação) -...|            Amadora|\n",
            "|      0|      1507-11|    STOPPED_AT|42|2566|38.766045|   2605|-9.297876|  2605_0_1|  2605_0|            SCHEDULED|     1635|      0.0| 172600|2024-11-22 22:27:46|2605_0_1|1|1|2230...|2024-11-22|Cacém (Estação) -...|             Sintra|\n",
            "|      0|    1_1320-11|    STOPPED_AT|41|1402| 38.69077|   1608|-9.316004|  1608_0_1|  1608_0|            SCHEDULED|     1363|      0.0| 120817|2024-11-22 22:27:57|1608_0_1_2200_222...|2024-11-22|Oeiras (Estação N...|            Cascais|\n",
            "|      0|    1_1320-11|    STOPPED_AT|41|1402| 38.69077|   1608|-9.316004|  1608_0_1|  1608_0|            SCHEDULED|     1363|      0.0| 120817|2024-11-22 22:27:57|1608_0_1_2200_222...|2024-11-22|Oeiras (Estação N...|             Oeiras|\n",
            "|    102|ESC_DU_EU2110|   INCOMING_AT|43|2016|  38.6474|   3023|-9.201875|  3023_0_1|  3023_0|            SCHEDULED|   EU2218| 9.166667| 020707|2024-11-22 22:27:24|3023_0_1_2200_222...|2024-11-22|Costa da Caparica...|             Almada|\n",
            "|    181|ESC_DU_EU1089| IN_TRANSIT_TO|43|2331|38.659004|   3510|-9.159439|  3510_0_1|  3510_0|            SCHEDULED|   EU1145|13.611111| 020506|2024-11-22 22:27:45|3510_0_1_2200_222...|2024-11-22|Cacilhas (Termina...|             Seixal|\n",
            "|    181|ESC_DU_EU1089| IN_TRANSIT_TO|43|2331|38.659004|   3510|-9.159439|  3510_0_1|  3510_0|            SCHEDULED|   EU1145|13.611111| 020506|2024-11-22 22:27:45|3510_0_1_2200_222...|2024-11-22|Cacilhas (Termina...|             Almada|\n",
            "|    319|    1_1404-11| IN_TRANSIT_TO|41|1168|38.701595|   1107|-9.236646|  1107_0_1|  1107_0|            SCHEDULED|     1499|7.7777777| 120269|2024-11-22 22:27:45|1107_0_1_2200_222...|2024-11-22|Algés (Estação) -...|             Oeiras|\n",
            "|     82|ESC_DU_EU1101| IN_TRANSIT_TO|43|2296|38.621395|   3112|-9.094603|  3112_0_1|  3112_0|            SCHEDULED|   EU1177| 8.333333| 140571|2024-11-22 22:27:48|3112_0_1_2200_222...|2024-11-22|Fogueteiro (Estaç...|             Seixal|\n",
            "+-------+-------------+--------------+-------+---------+-------+---------+----------+--------+---------------------+---------+---------+-------+-------------------+--------------------+----------+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ReadParquetExample\").getOrCreate()\n",
        "\n",
        "# Path to files\n",
        "parquet_vehicles_enriched = \"/content/lake/gold/vehicles_enriched\"\n",
        "\n",
        "\n",
        "class ETLTask():\n",
        "  def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "  def gold_vehicles(self):\n",
        "\n",
        "    #Read parquet files\n",
        "    df_vehicles_enriched = spark.read.parquet(parquet_vehicles_enriched)\n",
        "\n",
        "    df_vehicles_enriched_test = df_vehicles_enriched.dropDuplicates([\"line_id\"])\n",
        "\n",
        "\n",
        "    df_vehicles_enriched_by_municipality = df_vehicles_enriched_test.groupBy(\"municipality_name\").agg(\n",
        "      count(\"line_id\").alias(\"vehicle_count\"),\n",
        "      sum(\"speed\").alias(\"total_speed\")\n",
        "    ).orderBy(desc(col(\"total_speed\"))).limit(3)\n",
        "\n",
        "    df_vehicles_enriched_by_municipality_avg = df_vehicles_enriched.groupBy(\"municipality_name\").agg(\n",
        "      avg(\"speed\").alias(\"avg_speed\")\n",
        "    ).orderBy(desc(col(\"avg_speed\"))).limit(3)\n",
        "\n",
        "    df_vehicles_enriched_by_municipality.show()\n",
        "\n",
        "    df_vehicles_enriched_by_municipality_avg.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master('local').appName('Challenge 4').getOrCreate()\n",
        "\n",
        "    print(\"Starting Challenging 4\")\n",
        "    etl = ETLTask(spark)\n",
        "\n",
        "     # run tasks\n",
        "    print(\"Running Task\")\n",
        "    etl.gold_vehicles()\n"
      ],
      "metadata": {
        "id": "i9DTJMjyymOG",
        "outputId": "584969ec-a928-40ed-fa17-5dedf9319459",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Challenging 4\n",
            "Running Task\n",
            "+-----------------+-------------+------------------+\n",
            "|municipality_name|vehicle_count|       total_speed|\n",
            "+-----------------+-------------+------------------+\n",
            "|           Lisboa|           33|258.05555391311646|\n",
            "|           Seixal|           21| 129.7222217321396|\n",
            "|           Sintra|           22| 95.83333373069763|\n",
            "+-----------------+-------------+------------------+\n",
            "\n",
            "+-----------------+------------------+\n",
            "|municipality_name|         avg_speed|\n",
            "+-----------------+------------------+\n",
            "|         Alenquer| 8.333333015441895|\n",
            "|          Palmela|7.6984125546046664|\n",
            "|           Oeiras| 7.615740756193797|\n",
            "+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}