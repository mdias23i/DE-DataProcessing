{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdias23i/DE-DataProcessing/blob/main/spark/challenges/challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# CHALLENGE 1\n",
        "##  Implement INGESTION process\n",
        "- Set up path in the \"lake\"\n",
        "  - !mkdir -p /content/lake/bronze\n",
        "\n",
        "- Read data from API https://api.carrismetropolitana.pt/\n",
        "  - Endpoints:\n",
        "    - vehicles\n",
        "    - lines\n",
        "    - municipalities\n",
        "  - Use StructFields to enforce schema\n",
        "\n",
        "- Transformations\n",
        "  - vehicles\n",
        "    - create \"date\" extracted from \"timestamp\" column (format: date - yyyy-mm-dd or yyyymmdd)\n",
        "\n",
        "- Write data as PARQUET into the BRONZE layer (/content/lake/bronze)\n",
        "  - Partition \"vehicles\" by \"date\" column\n",
        "  - Paths:\n",
        "    - vehicles - path: /content/lake/bronze/vehicles\n",
        "    - lines - path: /content/lake/bronze/lines\n",
        "    - municipalities - path: /content/lake/bronze/municipalities\n",
        "  - Make sure there is only 1 single parquet created\n",
        "  - Use overwrite as write mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "c410e46c-4a50-43aa-926f-d0417c6280d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "\n",
        "class ETLFlow:\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def extract_from_api(self, url: str, schema: StructType = None):\n",
        "      response = requests.get(url)\n",
        "      rdd = spark.sparkContext.parallelize(response.json())\n",
        "      if schema:\n",
        "        df = spark.read.schema(schema).json(rdd)\n",
        "      else:\n",
        "        df = spark.read.json(rdd)\n",
        "      return df\n",
        "\n",
        "    def load(self, df: DataFrame, format: str, path: str, **kwargs) -> None:\n",
        "        df.write.mode(\"overwrite\").format(format).save(path)\n",
        "\n",
        "\n",
        "\n",
        "class ETLTask(ETLFlow):\n",
        "\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def ingestion_vehicles(self):\n",
        "      vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                                  StructField('block_id', StringType(), True),\n",
        "                                  StructField('current_status', StringType(), True),\n",
        "                                  StructField('id', StringType(), True),\n",
        "                                  StructField('lat', FloatType(), True),\n",
        "                                  StructField('line_id', StringType(), True),\n",
        "                                  StructField('lon', FloatType(), True),\n",
        "                                  StructField('pattern_id', StringType(), True),\n",
        "                                  StructField('route_id', StringType(), True),\n",
        "                                  StructField('schedule_relationship', StringType(), True),\n",
        "                                  StructField('shift_id', StringType(), True),\n",
        "                                  StructField('speed', FloatType(), True),\n",
        "                                  StructField('stop_id', StringType(), True),\n",
        "                                  StructField('timestamp', TimestampType(), True),\n",
        "                                  StructField('trip_id', StringType(), True)])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/vehicles\", schema = vehicle_schema)\n",
        "      df = df.withColumn('date', to_date(col('timestamp')))\n",
        "\n",
        "      (df\n",
        "        .repartition(1)\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .partitionBy(\"date\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/bronze/vehicles\")\n",
        "      )\n",
        "\n",
        "\n",
        "    def ingestion_lines(self):\n",
        "\n",
        "      lines_schema = StructType([\n",
        "                                    StructField(\"color\", StringType(), True),\n",
        "                                    StructField(\"facilities\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"id\", StringType(), True),\n",
        "                                    StructField(\"localities\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"long_name\", StringType(), True),\n",
        "                                    StructField(\"municipalities\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"patterns\", ArrayType(ArrayType(IntegerType())), True),\n",
        "                                    StructField(\"routes\", ArrayType(StringType()), True),\n",
        "                                    StructField(\"short_name\", StringType(), True),\n",
        "                                    StructField(\"text_color\", StringType(), True)\n",
        "                                ])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/lines\", schema = lines_schema)\n",
        "\n",
        "      (df\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/bronze/lines\")\n",
        "      )\n",
        "\n",
        "\n",
        "    def ingestion_municipalities(self):\n",
        "\n",
        "      municipalities_schema = StructType([\n",
        "                                    StructField(\"district_id\", StringType(), True),\n",
        "                                    StructField(\"district_name\", StringType(), True),\n",
        "                                    StructField(\"id\", StringType(), True),\n",
        "                                    StructField(\"name\", StringType(), True),\n",
        "                                    StructField(\"prefix\", StringType(), True),\n",
        "                                    StructField(\"region_id\", StringType(), True),\n",
        "                                    StructField(\"region_name\", StringType(), True)\n",
        "                                ])\n",
        "\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/municipalities\", schema = municipalities_schema)\n",
        "\n",
        "      (df\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .format(\"parquet\")\n",
        "        .save(path=\"/content/lake/bronze/municipalities\")\n",
        "      )\n",
        "\n",
        "    def cleansing_vehicles(self):\n",
        "      # Path to file\n",
        "      parquet_vehicles_file = \"/content/lake/bronze/vehicles\"\n",
        "      #Read vehicles parquet file\n",
        "      df_vehicles = spark.read.parquet(parquet_vehicles_file)\n",
        "\n",
        "\n",
        "      # Transformations\n",
        "      # rename \"lat\" and \"lon\" to \"latitude\" and \"longitude\" respectively\n",
        "      df_vehicles = df_vehicles.withColumnRenamed(\"lat\", \"latitude\")\n",
        "      df_vehicles = df_vehicles.withColumnRenamed(\"lon\", \"longitude\")\n",
        "      # remove possible duplicates\n",
        "      df_vehicles = df_vehicles.drop_duplicates()\n",
        "      # remove rows when the column CURRENT_STATUS is null\n",
        "      df_vehicles = df_vehicles.filter(df_vehicles['current_status'].isNotNull())\n",
        "\n",
        "\n",
        "      (df_vehicles\n",
        "          .repartition(1)\n",
        "          .write\n",
        "          .mode(\"overwrite\")\n",
        "          .partitionBy(\"date\")\n",
        "          .format(\"parquet\")\n",
        "          .save(path=\"/content/lake/silver/vehicles\")\n",
        "        )\n",
        "\n",
        "\n",
        "    def cleansing_lines(self):\n",
        "      parquet_lines_file = \"/content/lake/bronze/lines\"\n",
        "      #Read lines parquet file\n",
        "      df_lines = spark.read.parquet(parquet_lines_file)\n",
        "      # remove possible duplicates\n",
        "      df_lines = df_lines.drop_duplicates()\n",
        "      #remove rows when the column LONG_NAME is null\n",
        "      df_lines = df_lines.filter(df_lines['long_name'].isNotNull())\n",
        "\n",
        "      self.load(df=df_lines, format=\"parquet\", path=\"/content/lake/silver/lines\")\n",
        "\n",
        "\n",
        "\n",
        "    def cleansing_municipalities(self):\n",
        "      parquet_municipalities_file = \"/content/lake/bronze/municipalities\"\n",
        "      #Read municipalities parquet file\n",
        "      df_municipalities = spark.read.parquet(parquet_municipalities_file)\n",
        "      # remove possible duplicates\n",
        "      df_municipalities = df_municipalities.drop_duplicates()\n",
        "      #remove rows when the columns NAME or DISTRICT_NAME are null\n",
        "      df_municipalities = df_municipalities.filter((df_municipalities['name'].isNotNull()) & (col(\"district_name\").isNotNull()))\n",
        "\n",
        "      self.load(df=df_municipalities, format=\"parquet\", path=\"/content/lake/silver/municipalities\")\n",
        "\n",
        "\n",
        "    def enrich_vehicles(self):\n",
        "      # Path to files\n",
        "      parquet_vehicles_file = \"/content/lake/silver/vehicles\"\n",
        "      parquet_lines_file = \"/content/lake/silver/lines\"\n",
        "      parquet_municipalities_file = \"/content/lake/silver/municipalities\"\n",
        "      #Read parquet files\n",
        "      df_vehicles = spark.read.parquet(parquet_vehicles_file)\n",
        "      df_lines = spark.read.parquet(parquet_lines_file)\n",
        "      df_municipalities = spark.read.parquet(parquet_municipalities_file)\n",
        "\n",
        "\n",
        "      df_lines = df_lines.withColumn('municipality_id', explode(df_lines['municipalities']))\n",
        "\n",
        "      df_vehicles.createOrReplaceTempView(\"vehicles\")\n",
        "      df_lines.createOrReplaceTempView(\"lines\")\n",
        "      df_municipalities.createOrReplaceTempView(\"municipalities\")\n",
        "\n",
        "\n",
        "      query = \"\"\"\n",
        "                SELECT\n",
        "                    vehicles.*,  -- all columns vehicles\n",
        "                    lines.long_name AS line_name,  --  lines.long_name to line_name\n",
        "                    municipalities.name AS municipality_name  --  municipalities.name to municipality_name\n",
        "                FROM\n",
        "                    vehicles\n",
        "                JOIN\n",
        "                    lines ON vehicles.line_id = lines.id  -- Join vehicles and lines\n",
        "                JOIN\n",
        "                    municipalities ON lines.municipality_id = municipalities.id  -- Join municipalities\n",
        "              \"\"\"\n",
        "\n",
        "      result_df = spark.sql(query)\n",
        "\n",
        "      (result_df\n",
        "          .repartition(1)\n",
        "          .write\n",
        "          .mode(\"overwrite\")\n",
        "          .partitionBy(\"date\")\n",
        "          .format(\"parquet\")\n",
        "          .save(path=\"/content/lake/gold/vehicles_enriched\")\n",
        "        )\n",
        "\n",
        "    def gold_vehicles(self):\n",
        "      parquet_vehicles_enriched = \"/content/lake/gold/vehicles_enriched\"\n",
        "      #Read parquet files\n",
        "      df_vehicles_enriched = spark.read.parquet(parquet_vehicles_enriched)\n",
        "\n",
        "      df_vehicles_enriched_test = df_vehicles_enriched.dropDuplicates([\"line_id\"])\n",
        "\n",
        "\n",
        "      df_vehicles_enriched_by_municipality = df_vehicles_enriched_test.groupBy(\"municipality_name\").agg(\n",
        "        count(\"line_id\").alias(\"vehicle_count\"),\n",
        "        sum(\"speed\").alias(\"total_speed\")\n",
        "      ).orderBy(desc(col(\"total_speed\"))).limit(3)\n",
        "\n",
        "      df_vehicles_enriched_by_municipality_avg = df_vehicles_enriched.groupBy(\"municipality_name\").agg(\n",
        "        avg(\"speed\").alias(\"avg_speed\")\n",
        "      ).orderBy(desc(col(\"avg_speed\"))).limit(3)\n",
        "\n",
        "      print(\"The 3 top municipalities by vehicles routes:\")\n",
        "      df_vehicles_enriched_by_municipality.show()\n",
        "\n",
        "      print(\"The top 3 municipalities with higher vehicle speed on average:\")\n",
        "      df_vehicles_enriched_by_municipality_avg.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder.master('local').appName('Challenges').getOrCreate()\n",
        "\n",
        "    print(\"Starting Challenges\")\n",
        "    etl = ETLTask(spark)\n",
        "\n",
        "    # run tasks\n",
        "    print(\"Running Task - Ingestion Vehicles\")\n",
        "    etl.ingestion_vehicles()\n",
        "    print(\"Running Task - Ingestion Lines\")\n",
        "    etl.ingestion_lines()\n",
        "    print(\"Running Task - Ingestion municipalities\")\n",
        "    etl.ingestion_municipalities()\n",
        "    print(\"Running Task - Cleansing Vehicles\")\n",
        "    etl.cleansing_vehicles()\n",
        "    print(\"Running Task - Cleansing Lines\")\n",
        "    etl.cleansing_lines()\n",
        "    print(\"Running Task - Cleansing Municipalities\")\n",
        "    etl.cleansing_municipalities()\n",
        "    print(\"Running Task - Enrich Vehicles\")\n",
        "    etl.enrich_vehicles()\n",
        "    print(\"Running Task - Gold Vehicles\")\n",
        "    etl.gold_vehicles()"
      ],
      "metadata": {
        "id": "R47csbbcYuFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}